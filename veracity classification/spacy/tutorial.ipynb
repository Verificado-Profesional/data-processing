{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dante/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dante/.pyenv/versions/3.12.0/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/dante/.pyenv/versions/3.12.0/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n",
      "mundo\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hola mundo!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Etiquetado Gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ella PRON\n",
      "comió VERB\n",
      "pizza PROPN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "doc = nlp(\"Ella comió pizza\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "EE.UU. LOC\n",
      "iPhone MISC\n",
      "Galaxy Note 9 MISC\n"
     ]
    }
   ],
   "source": [
    "# Procesa un texto\n",
    "doc = nlp(\n",
    "    \"Apple es la marca que más satisfacción genera en EE.UU., \"\n",
    "    \"pero el iPhone, fue superado por el Galaxy Note 9\"\n",
    ")\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la etiqueta de la entidad\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "River Plate ORG\n",
      "Argentina LOC\n"
     ]
    }
   ],
   "source": [
    "# Procesa un texto\n",
    "doc = nlp(\n",
    "    \"River Plate es el equipo mas grande de Argentina \"\n",
    ")\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la etiqueta de la entidad\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern Maching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adidas zx\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "matcher.add(\"ADIDAS_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Obtén el span resultante\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"copa\"},\n",
    "    {\"LOWER\": \"mundial\"},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "doc = nlp(\"2014 Copa Mundial FIFA: Alemania ganó!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"comer\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "doc = nlp(\"Camila prefería comer sushi. Pero ahora está comiendo pasta.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9565357104409163886\n",
      "gato\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Yo tengo un gato\")\n",
    "\n",
    "# Busca el hash para la palabra \"gato\"\n",
    "gato_hash = nlp.vocab.strings[\"gato\"]\n",
    "print(gato_hash)\n",
    "\n",
    "# Busca el gato_hash para obtener el string\n",
    "gato_string = nlp.vocab.strings[gato_hash]\n",
    "print(gato_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc & Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Crea un span manualmente\n",
    "span = Span(doc, 1, 3)\n",
    "\n",
    "# Crea un span con un label\n",
    "span_with_label = Span(doc, 1, 3, label=\"SALUDO\")\n",
    "\n",
    "# Añade el span a los doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9513663710080219\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Me gusta la comida rápida\")\n",
    "doc2 = nlp(\"Me gusta la pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1363773817075813\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Me gusta la pizza\")\n",
    "token = nlp(\"jabón\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.58136    0.037496   0.66934    2.7966    -0.023352   0.39145\n",
      "  0.55106    0.2597     2.6257     3.1932    -0.49274    0.084971\n",
      "  0.083045  -1.1788    -0.11184    0.052108  -0.563      0.2155\n",
      " -1.5244    -1.9768    -1.6693    -0.85393    0.89011   -0.99332\n",
      "  1.7136    -1.7498    -1.5536     0.44981    0.76886    1.298\n",
      "  0.094683  -0.07842    1.1843    -1.5305    -0.44663    1.3727\n",
      "  1.2239    -1.4967     0.75916    0.70923    1.4964     0.56073\n",
      " -1.6018    -0.91338   -2.0583     1.1208    -0.86255    0.76231\n",
      "  0.60926   -1.0933    -2.0223    -1.2327     0.24917    0.95122\n",
      " -1.0975    -0.83044   -1.4911    -0.79706   -0.23831    0.10205\n",
      " -0.44532    1.0172     1.2452    -2.0414    -0.39335    1.149\n",
      " -0.0094314  1.5695    -2.2982     1.2705     0.59918    0.95636\n",
      "  0.20392    0.35687    1.6167    -0.80719   -0.52339    0.68923\n",
      " -0.3944    -3.0173     1.0634    -3.4248    -0.29587   -1.2832\n",
      " -1.6083     0.74691   -0.11828    1.4702     0.16136   -1.4201\n",
      "  0.70638   -0.072624  -1.4466     0.72344    0.078114   1.2596\n",
      " -1.8129    -0.65629   -0.80641   -1.1449    -2.545      0.85491\n",
      "  0.54189    1.8337    -0.20536    0.12252   -2.2903    -0.1698\n",
      "  0.51621   -2.0066     0.89327   -0.91274   -0.97247    1.1017\n",
      "  2.8891    -0.81364    0.022315   1.5598    -1.8567    -1.3784\n",
      "  1.2099     0.5902    -1.3276     0.24976    0.94643    0.31263\n",
      " -1.7238    -1.5393     1.7214    -0.83761    1.0829     1.0269\n",
      " -1.2177    -0.079954   0.4957     0.7978    -1.3404     1.5048\n",
      "  0.42446    2.1174    -0.72877    0.57396    0.59383   -0.24617\n",
      "  1.004      1.1322    -0.6847     0.63397   -1.3758     1.126\n",
      "  0.087296   0.17559    0.66427   -0.39574    0.36193    2.6496\n",
      "  1.4788    -0.20837   -0.13546   -1.6081     0.76137    0.40225\n",
      " -2.3385    -0.37009    1.5414    -0.37543   -0.74023    0.37674\n",
      " -1.5544    -0.43297   -1.5797     1.0251     1.283     -1.8788\n",
      " -0.084524   0.76776   -1.4217     1.289     -0.063222   0.56392\n",
      " -0.43796   -0.52452   -0.35984    0.91657    1.5457    -1.3059\n",
      "  1.0045    -1.6402     0.74614   -0.88665   -0.70848    0.43385\n",
      " -0.61166   -0.6628     0.40492   -0.81584   -0.10816    1.4434\n",
      " -1.3407    -0.12776   -0.35255   -1.6831    -2.9178    -1.2738\n",
      "  0.95345   -0.20384    0.70905   -1.1009    -0.98886   -0.78085\n",
      " -0.68736    0.68719    0.39532    0.7868     0.9388     0.26213\n",
      "  0.51041    1.2448     0.39292    0.56005    0.16127   -0.26196\n",
      " -0.52357   -0.82289    1.0116    -0.036065   2.4301     0.44541\n",
      "  0.30908   -0.5454    -0.66264   -1.1558     0.15025    0.60381\n",
      " -1.9326     0.55429   -0.80461    0.525     -0.37794    1.2012\n",
      " -0.41166   -0.68876   -1.6461    -1.3226    -0.057329   1.8578\n",
      "  1.0736    -0.2627    -1.0381     0.14438   -1.5073     0.6385\n",
      "  0.14405    1.7461     1.14       0.1884     0.25404    0.3859\n",
      " -0.55736   -0.54779    1.6302    -0.054984   1.1899    -0.44067\n",
      " -1.0749    -0.66979   -1.4983    -0.40309    1.7218     0.64941\n",
      " -1.0417    -1.2201     0.025452  -1.3249     0.8728    -0.88773\n",
      "  0.36258    0.43347    1.3337    -0.68388    1.0006    -1.3476\n",
      " -2.2166    -0.44404   -1.2076     1.2248     1.0834     0.10092\n",
      " -0.22438   -0.3598     1.3011     1.2147     0.2494    -0.39024\n",
      " -0.44607    0.41497    0.6931    -0.5522     0.51031    0.058363 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tengo una manzana\")\n",
    "# Accede al vector a través del atributo token.vector\n",
    "print(doc[2].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761564889481511\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Me gustan los gatos\")\n",
    "doc2 = nlp(\"Me desagradan los gatos\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrase Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span resultante: labrador dorado\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"labrador dorado\")\n",
    "matcher.add(\"PERRO\", [pattern])\n",
    "doc = nlp(\"Tengo un labrador dorado\")\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Obtén el span resultante\n",
    "    span = doc[start:end]\n",
    "    print(\"span resultante:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f294c0c8d70>),\n",
       " ('morphologizer',\n",
       "  <spacy.pipeline.morphologizer.Morphologizer at 0x7f294743e090>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f2947296ea0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f294701a2d0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.es.lemmatizer.SpanishLemmatizer at 0x7f29472f8250>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f2946f87df0>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Imprime la longitud del doc en pantalla\n",
    "    print(\"longitud del Doc:\", len(doc))\n",
    "    # Devuelve el objeto doc\n",
    "    return doc\n",
    "\n",
    "# Añade el componente al primer lugar del pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Imprime los nombres de los componentes del pipeline\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del Doc: 4\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"¡Hola Mundo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio escribirás un componente personalizado que use el PhraseMatcher para encontrar nombres de animales en el documento y añada los spans resultantes a los doc.ents. En la variable matcher ya se creó un PhraseMatcher con los patrones de animales.\n",
    "\n",
    "Define el componente personalizado y aplica el matcher al doc.\n",
    "Crea un Span para cada resultado, asígnale el label ID para \"ANIMAL\" y sobrescribe los doc.ents con los nuevos spans.\n",
    "Añade el nuevo componente al pipeline después del componente \"ner\".\n",
    "Procesa el texto e imprime en pantalla el texto de la entidad y los entity labels de las entidades en doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del Doc: 2\n",
      "longitud del Doc: 1\n",
      "longitud del Doc: 1\n",
      "longitud del Doc: 3\n",
      "patrones_de_animales: [labrador dorado, gato, tortuga, oso de anteojos]\n",
      "['custom_component', 'tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "longitud del Doc: 12\n",
      "[('tortuga', 'ANIMAL'), ('oso de anteojos', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "animals = [\"labrador dorado\", \"gato\", \"tortuga\", \"oso de anteojos\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"patrones_de_animales:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Aplica el matcher al doc\n",
    "    matches = matcher(doc)\n",
    "    # Crea un Span para cada resultado y asígnales el label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Sobrescribe los doc.ents con los spans resultantes\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline después del componente \"ner\"\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto y el label\n",
    "# de los doc.ents\n",
    "doc = nlp(\"Hoy vimos una tortuga y un oso de anteojos en nuestra caminata\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extension de Atributos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las clases globales\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Añade extensiones para el Doc, Token y Span\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.title = \"Mi documento\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del Doc: 5\n",
      "True - azul\n"
     ]
    }
   ],
   "source": [
    "# Define la función getter\n",
    "def get_is_color(token):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Añade una extensión en el Token con getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color,force=True)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del Doc: 5\n",
      "True - cielo es azul\n",
      "False - El cielo\n"
     ]
    }
   ],
   "source": [
    "# Define la función getter\n",
    "def get_has_color(span):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Añade una extensión en el Span con getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color,force=True)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extension de Metodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del Doc: 5\n",
      "True - azul\n",
      "False - nube\n"
     ]
    }
   ],
   "source": [
    "# Define un método con argumentos\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Añade una extensión en el Doc con el método\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc._.has_token(\"azul\"), \"- azul\")\n",
    "print(doc._.has_token(\"nube\"), \"- nube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invertido: sadoT\n",
      "invertido: sal\n",
      "invertido: senoicazilareneg\n",
      "invertido: nos\n",
      "invertido: saslaf\n",
      "invertido: ,\n",
      "invertido: odneyulcni\n",
      "invertido: atse\n",
      "invertido: .\n"
     ]
    }
   ],
   "source": [
    "nlp_blank = spacy.blank(\"es\")\n",
    "\n",
    "# Define la función getter que toma un token y devuelve su texto al revés\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Token, \"reversed\", con\n",
    "# el getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el atributo \"reversed\"\n",
    "# para cada token\n",
    "doc = nlp_blank(\"Todas las generalizaciones son falsas, incluyendo esta.\")\n",
    "for token in doc:\n",
    "    print(\"invertido:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio combinarás la extensión de atributos personalizados con las predicciones del modelo y crearás un getter de atributo que devuelve una URL de búsqueda de Wikipedia si el span es una persona, organización o lugar.\n",
    "\n",
    "Completa el getter get_wikipedia_url para que solo devuelva la URL si el label del span está en la lista de labels.\n",
    "Añade la extensión del Span, \"wikipedia_url\", usando el getter get_wikipedia_url.\n",
    "Itera sobre las entidades en el doc y devuelve sus URLs de Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie https://es.wikipedia.org/w/index.php?search=David_Bowie\n",
      "Alemania https://es.wikipedia.org/w/index.php?search=Alemania\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Obtén la URL de Wikipedia si el span tiene uno de los siguientes labels\n",
    "    if span.label_ in (\"PER\", \"ORG\", \"LOC\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://es.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Añade la extensión del Span, wikipedia_url, usando el getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Antes de finalizar 1976, el interés de David Bowie en la \"\n",
    "    \"floreciente escena musical alemana, le llevó a mudarse a \"\n",
    "    \"Alemania para revitalizar su carrera.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la URL de Wikipedia de la entidad\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasar contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es un texto 15\n",
      "y otro texto 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Esto es un texto\", {\"id\": 1, \"numero_pagina\": 15}),\n",
    "    (\"y otro texto\", {\"id\": 2, \"numero_pagina\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"numero_pagina\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"numero_pagina\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"Esto es un texto\", {\"id\": 1, \"numero_pagina\": 15}),\n",
    "    (\"y otro texto\", {\"id\": 2, \"numero_pagina\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.numero_pagina = context[\"numero_pagina\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usar solo el tokenizer si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp.make_doc(\"¡Hola Mundo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deshabilitar pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "with nlp.select_pipes(disable=[\"senter\", \"parser\"]):\n",
    "    # Procesa el texto e imprime las entidades en pantalla\n",
    "    doc = nlp(\"text\")\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desarrollo nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "docs= [\"Hola\",\"Mundo\"]\n",
    "random.shuffle(docs)\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs = docs[len(docs) // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span, DocBin\n",
    "# Crea y guarda una colección de docs para entrenamiento\n",
    "train_docbin = DocBin(docs=train_docs)\n",
    "train_docbin.to_disk(\"./train.spacy\")\n",
    "# Crea y guarda una colección de docs para evaluación\n",
    "dev_docbin = DocBin(docs=dev_docs)\n",
    "dev_docbin.to_disk(\"./dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generar una configuración\n",
    "- spaCy puede auto-generar un archivo de configuración default por ti\n",
    "- interactivo wodget de inicio en los archivos\n",
    "- init config comando en CLI\n",
    "```\n",
    " python -m spacy init config ./config.cfg --lang es --pipeline ner\n",
    "```\n",
    "- init config: el comando a correr\n",
    "- config.cfg: ruta de salida de la configuración generada\n",
    "- --lang: la clase del idioma del pipeline, p. ej. es para español\n",
    "- --pipeline: nombres, separados con comas, de los componentes a incluir\n",
    "\n",
    "python -m spacy init config ./config.cfg --lang es --pipeline textcat --optimize accuracy spacy_categorization_pipeline.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entrenar un Pipeline\n",
    "- todo lo que necesitas es el config.cfg y los datos de entrenamiento y desarrollo\n",
    "- los ajustes de configuración pueden modificarse en la línea de comando\n",
    "```\n",
    "$ python -m spacy train ./config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy\n",
    "```\n",
    "- train: el comando a correr\n",
    "- config.cfg: ruta de archivo de configuración\n",
    "- --output: ruta del directorio de salida para guardar el pipeline entrenado\n",
    "- --paths.train: anular con ruta a los datos de entrenamiento\n",
    "- --paths.dev: anular con ruta a los datos de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cargando un pipeline entrenado\n",
    "\n",
    "- la salida de datos después de un pipeline normal de spaCy\n",
    "- model-last: el pipeline entrenado más recientemente\n",
    "- model-best: el pipeline mejor entrenado\n",
    "- cárgalo con spacy.load\n",
    "```\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"/ruta/de/arhcivo/model-best\")\n",
    "doc = nlp(\"iPhone 11 vs iPhone 8: Cuál es la diferencia?\")\n",
    "print(doc.ents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Empaquetando tu pipeline\n",
    "\n",
    "- spacy package: crea un paquete de Python instalable que contiene tu pipeline\n",
    "- es fácil de versionar y de \n",
    "```\n",
    "$ python -m spacy package /path/to/output/model-best ./packages --name mi_pipeline --version 1.0.0\n",
    "```\n",
    "```\n",
    "$ cd ./packages/es_mi_pipeline-1.0.0\n",
    "$ pip install dist/es_mi_pipeline-1.0.0.tar.gz\n",
    "```\n",
    "- Carga y utiliza el pipeline después de instalarlo:\n",
    "- nlp = spacy.load(\"es_mi_pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit ('3.12.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "825df0fcbad1e6ed7e11c1d53fafc04f4a658ad54114578619c88a90b04ca641"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
